---
title       : Capstone Milestone Presentation
subtitle    : Data Science Specialization Capstone | Summer 2015 
author      : Paul Fornia
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---
    
## Input Data
    
Input tables provide data from three different types of internet text sources:    

1. Blogs
2. News
3. Twitter

--- .class #id 

## Length of Lines

Length of lines (in number of characters) varies by source type.

```{r, echo = FALSE, cache=TRUE, warning=FALSE}
rawDataFolder <- "../data/final/en_US/"

blogsDataRaw <- readLines(paste0(rawDataFolder, "en_US.blogs.txt"))
newsDataRaw <- readLines(paste0(rawDataFolder, "en_US.news.txt"))
twitterDataRaw <- readLines(paste0(rawDataFolder, "en_US.twitter.txt"))

blogLineNchar <- nchar(blogsDataRaw)
newsLineNchar <- nchar(newsDataRaw) 
twitterLineNchar <- nchar(twitterDataRaw) 

set.seed(1)

dfBlogNchar <- data.frame(nChar = sample(blogLineNchar, 10000), sourceType = "Blog")
dfNewsNchar <- data.frame(nChar = sample(newsLineNchar, 10000), sourceType = "News")
dfTwitterNchar <- data.frame(nChar = sample(twitterLineNchar, 10000), sourceType = "Twitter")

dfLineNchar <- rbind(dfBlogNchar, dfNewsNchar, dfTwitterNchar)

library(ggplot2)

ggplot(dfLineNchar, aes(log(nChar), fill = sourceType)) + geom_histogram(position = "dodge", binwidth = 0.25)

```

--- .class #id 

## Most used "non-trivial" words

Using the "tm" package, I:

1. Filter out very common "stopwords"

2. Change all data to lower case

3. Remove most punctuation (some work still needed)

4. Remove profanity

5. Produce Wordclouds of Top 100 common words by source type


```{r, results = FALSE, echo = FALSE, cache=TRUE, warning=FALSE}
library(tm)

## source: https://en.wikipedia.org/wiki/Seven_dirty_words   
##  ...plus a little creativity.

getTop40 <- function(testTMData, numberOut = 40){
    allText <- paste(testTMData, collapse = " ")
    allTextVector <- VectorSource(allText)
    allTextCorpus <- Corpus(allTextVector)
    
    allTextCorpus1 <- tm_map(allTextCorpus, content_transformer(tolower))
    allTextCorpus2 <- tm_map(allTextCorpus1, removePunctuation)
    ## removing special characters still needs some work!
    allTextCorpus3 <- tm_map(allTextCorpus2, removeWords, c("â€œ", "â€™", "â€“", "itâ€™s"))
    allTextCorpus4 <- tm_map(allTextCorpus3, removeWords, stopwords("english"))
    
    profanityList <- c("asshole", "assholes", "bitch", "bitches", "bitching", "cocksucker", "cocksuckers", "cunt", "cunts", "faggot", "faggots", "fuck", "fuckers", "fucking", "fucks", "goddamn", "goddamned", "goddamnit", "jackass", "jackasses", "motherfucker", "motherfuckers", "niger", "nigers", "piss", "pissing", "shit", "shithole", "shits")
    
    allTextCorpus5 <- tm_map(allTextCorpus4, removeWords, profanityList)
    
    
    dtm <- DocumentTermMatrix(allTextCorpus5)
    dtm2 <- as.matrix(dtm)
    dim(dtm2)
    
    frequency <- sort(colSums(dtm2), decreasing = TRUE)
    largeFreq <- frequency[1:numberOut]
    return(largeFreq)
}

top100blogs <- getTop40(sample(blogsDataRaw, 1000), numberOut = 100)
top100news <- getTop40(sample(newsDataRaw, 1000), numberOut = 100)
top100twitter <- getTop40(sample(twitterDataRaw, 1000), numberOut = 100)

library(wordcloud)

set.seed(100)
```

--- .class #id 

## Top 100 words in Blogs

```{r, echo=FALSE, cache=TRUE}
wordcloud(names(top100blogs), top100blogs, colors = 1:100, random.color = TRUE)
```

--- .class #id 

## Top 100 words in the News
```{r, echo=FALSE, cache=TRUE}
wordcloud(names(top100news), top100news, colors = 1:100, random.color = TRUE)
```

--- .class #id 

## Top 100 words on Twitter

```{r, echo=FALSE, cache=TRUE}
wordcloud(names(top100twitter), top100twitter, colors = 1:100, random.color = TRUE)
```

--- .class #id 

## Next Steps, Simple
Create simplest predictive algorithm:

1. Given a string of text, take last 3 words typed. If sufficient 4-tuples exist starting with these three words, suggest the 4th word of the most common 4-tuple.

2. If insufficent such 4-tuples exist, take only last 2 words in typed test data and repeat with all 3-tuple.

3. If insufficent 3-tuples, repeat with last word typed and doubles.

4. If last word is unusual or non-existent in training data, suggest single most common word.

--- .class #id 

## Next Steps, Advanced.
As I build my application, I'll make improvements as they become apparent. Some examples may include:

* Perform better filtering of special characters, punctuation.

* Incorporate parts of speech, and separate training data by independent and dependent clauses (e.g., don't count a n-tuple that spans two sentences).

* Any other sorting of topics beyond just the blog/news/twitter level. For instance, if blog post is identified as a cooking recipe, then use appropriate training data.